import streamlit as st
import pandas as pd
import os
import hashlib

READ_DB = "read_status.csv"  # å´è»Šæª”ï¼Œç”¨ä¾†ä¿å­˜å·²è®€ç‹€æ…‹ï¼ˆURLç‚ºå”¯ä¸€éµï¼‰

# ========== è®€å¯«å·²è®€ç‹€æ…‹ ==========
def ensure_read_db():
    """è‹¥ read_status.csv ä¸å­˜åœ¨å°±å»ºç«‹ç©ºæª”"""
    if not os.path.exists(READ_DB):
        pd.DataFrame({"ItemKey": [], "Read": []}).to_csv(READ_DB, index=False)

def _dedup_by_itemkey(df: pd.DataFrame) -> pd.DataFrame:
    if "ItemKey" in df.columns:
        # ä¿ç•™æœ€å¾Œä¸€æ¬¡å¯«å…¥çš„ç‹€æ…‹
        df = df.drop_duplicates(subset=["ItemKey"], keep="last")
    return df

def load_read_db() -> pd.DataFrame:
    ensure_read_db()
    try:
        db = pd.read_csv(READ_DB, dtype={"ItemKey": str})
        if "Read" not in db.columns:
            db["Read"] = False
        db = _dedup_by_itemkey(db)
        return db
    except Exception:
        # å£æª”æ™‚è‡ªå‹•é‡å»º
        empty = pd.DataFrame({"ItemKey": [], "Read": []})
        empty.to_csv(READ_DB, index=False)
        return empty

def save_read_db(db: pd.DataFrame):
    db = _dedup_by_itemkey(db)
    db.to_csv(READ_DB, index=False)

def mark_read(item_key: str, read_state: bool):
    db = load_read_db()
    if item_key in db["ItemKey"].values:
        db.loc[db["ItemKey"] == item_key, "Read"] = read_state
    else:
        db = pd.concat([db, pd.DataFrame({"ItemKey": [item_key], "Read": [read_state]})], ignore_index=True)
    save_read_db(db)

def stable_key(prefix: str, s: str) -> str:
    """æŠŠä»»æ„å­—ä¸² s è½‰ç‚ºç©©å®šã€å®‰å…¨çš„ widget key"""
    h = hashlib.md5(s.encode("utf-8"), usedforsecurity=False).hexdigest()
    return f"{prefix}_{h}"

# ========== è¼‰å…¥ä¸»è³‡æ–™ ==========
def load_data(file_path):
    try:
        df = pd.read_excel(file_path)
        required_columns = ['Date', 'Author', 'Title', 'Subtitle', 'URL', 'Category (20-class)']
        if not all(col in df.columns for col in required_columns):
            st.error("Excelæª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ï¼šDate, Author, Title, Subtitle, URL, Category (20-class)")
            return None

        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        # ç”¨ URL ç•¶å”¯ä¸€éµï¼ˆå»ºè­°ï¼‰
        df["ItemKey"] = df["URL"].astype(str)

        # è‹¥æƒ³æ”¹ç”¨ Author+Title ç•¶ keyï¼Œæ”¹ç”¨ä¸‹åˆ—é€™è¡Œï¼š
        # df["ItemKey"] = (df["Author"].astype(str) + " || " + df["Title"].astype(str)).str.strip()

        # åˆä½µå·²è®€ç‹€æ…‹
        read_db = load_read_db()
        df = df.merge(read_db, how="left", on="ItemKey")
        df["Read"] = df["Read"].fillna(False).astype(bool)
        return df
    except Exception as e:
        st.error(f"è®€å–æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return None

# ========== åˆ†é é¡¯ç¤º ==========
def display_paginated_articles(df):
    if df.empty:
        st.info("ç„¡æ–‡ç« ã€‚")
        return

    if 'page' not in st.session_state:
        st.session_state.page = 0

    items_per_page = 8
    total_pages = (len(df) + items_per_page - 1) // items_per_page
    # è‹¥ç¯©é¸è®Šæ›´é€ æˆé ç¢¼è¶…ç•Œï¼Œä¿éšªæ”¶å›æœ€å¾Œä¸€é 
    st.session_state.page = min(st.session_state.page, max(total_pages - 1, 0))

    start = st.session_state.page * items_per_page
    end = start + items_per_page
    page_df = df.iloc[start:end]

    # CSSï¼šæ·¡åŒ–å·²è®€
    st.markdown("""
    <style>
    .stMarkdown div { margin-top: -10px; margin-bottom: -10px; }
    .read-dim { opacity: 0.45; }
    </style>
    """, unsafe_allow_html=True)

    for _, row in page_df.iterrows():
        wrapper_class = "read-dim" if row.get("Read", False) else ""
        title_html = f"<font size=5>â™¦ <b>{row['Title']}</b></font>"
        subtitle_html = f"{row['Subtitle']}" if pd.notna(row['Subtitle']) else ""
        date_str = row['Date'].strftime('%Y-%m-%d') if pd.notna(row['Date']) else 'N/A'
        link_html = (
            f"{row['Author']},  {date_str}, "
            f"<a href='{row['URL']}' target='_blank' style='margin-right:15px'>ğŸ”—å…¨æ–‡é€£çµ</a>"
            f"<a href='https://freedium.cfd/{row['URL']}' target='_blank'>ğŸ”—ç ´è§£é€£çµ</a>"
            )
        st.markdown(f"<div class='{wrapper_class}'>{title_html}</div>", unsafe_allow_html=True)
        if subtitle_html:
            st.markdown(f"<div class='{wrapper_class}'>{subtitle_html}</div>", unsafe_allow_html=True)
        st.markdown(f"<div class='{wrapper_class}'>{link_html}</div>", unsafe_allow_html=True)

        # äº’å‹•æŒ‰éˆ•ï¼šæ¨™è¨˜ç‚ºå·²å ±å‘Š / å–æ¶ˆå·²å ±å‘Šï¼ˆä½¿ç”¨ç©©å®šé›œæ¹Š keyï¼‰
        col_a, col_b = st.columns([3, 3])
        btn_key_read = stable_key("btn_read", str(row['ItemKey']))
        if not row.get("Read", False):
            if col_a.button("æ¨™ç¤ºç‚ºå·²å ±å‘Š", key=btn_key_read):
                mark_read(row['ItemKey'], True)
                st.rerun()
        else:
            if col_a.button("å–æ¶ˆå·²å ±å‘Š", key=btn_key_read):
                mark_read(row['ItemKey'], False)
                st.rerun()

        st.markdown("---")

    col1, col2, col3 = st.columns([4, 1, 1])
    with col2:
        if st.button("ä¸Šä¸€é ") and st.session_state.page > 0:
            st.session_state.page -= 1
            st.rerun()
    with col3:
        if st.button("ä¸‹ä¸€é ") and st.session_state.page < total_pages - 1:
            st.session_state.page += 1
            st.rerun()
    st.write(f"é ç¢¼: {st.session_state.page + 1} / {total_pages}")

# ========== ä¸»ç¨‹å¼ ==========
st.title("Medium Digest-Web")

file_path = 'Medium digest (classified).xlsx'
df = load_data(file_path)


if df is not None:
    # ===== é¡åˆ¥æ¸…å–® =====
    ai_categories = [
        "Agentic AI & AI Agents",
        "Retrieval-Augmented Generation (RAG)",
        "Multimodal AI (Vision/Audio/Video + Language)",
        "Prompt Engineering & In-Context Learning",
        "Fine-tuning & Embeddings",
        "Large Language Models (LLM)",
        "Natural Language Processing (non-LLM)",
        "Computer Vision (CV)",
        "Speech & Audio AI",
        "Deep Learning (non-LLM)",
        "Machine Learning (Classical)",
        "AI Algorithm",
        "AI Evaluation & Metrics",
        "AI Infrastructure, MLOps & Frameworks",
        "AI Applications (Business/Dev/Productivity)",
        "AI Policy, Governance & Safety"
    ]

    non_ai_display_categories = [
        "Data Science & Statistics",
        "Software Engineering & Programming",
        "Technology/Science",
        "Finance/Economics/Business",
        "Society/Culture/Other"
    ]

    # ===== ç‹€æ…‹ï¼ˆå·²å ±å‘Š / æœªå ±å‘Šï¼‰ =====
    st.sidebar.subheader("ç‹€æ…‹")
    total_read = int(df["Read"].sum())
    total_unread = int((~df["Read"]).sum())
    status_options = [
        f"å…¨éƒ¨ï¼ˆ{len(df)}ï¼‰",
        f"åƒ…æœªå ±å‘Šï¼ˆ{total_unread}ï¼‰",
        f"åƒ…å·²å ±å‘Šï¼ˆ{total_read}ï¼‰"
    ]
    status_choice = st.sidebar.radio(
        "ä¾ç‹€æ…‹ç¯©é¸",
        status_options,
        index=0,
        key="status_filter"
    )
    st.sidebar.markdown("---")
    # ===== å´é‚Šæ¬„ï¼šåˆ†é¡ =====
    st.sidebar.header("æ–‡ç« åˆ†é¡")
    selected_labels = []

    unique_cats = set(df['Category (20-class)'].unique())

    st.sidebar.subheader("AIï¼ˆ15 é¡ï¼‰")
    ai_select_all = st.sidebar.checkbox("å…¨é¸ AI", key="ai__all")
    prev_ai_all = st.session_state.get("ai__all_prev", False)
    if ai_select_all != prev_ai_all:
        for label in ai_categories:
            if label in unique_cats:
                st.session_state[f"ai_{label}"] = ai_select_all
        st.session_state["ai__all_prev"] = ai_select_all

    for label in ai_categories:
        if label in unique_cats:
            cnt = (df['Category (20-class)'] == label).sum()
            if st.sidebar.checkbox(
                f"{label}ï¼ˆ{cnt} ç¯‡æ–‡ç« ï¼‰",
                key=f"ai_{label}",
                value=st.session_state.get(f"ai_{label}", False)
            ):
                selected_labels.append(label)

    st.sidebar.markdown("---")

    st.sidebar.subheader("é AIï¼ˆ5 é¡ï¼‰")
    non_ai_select_all = st.sidebar.checkbox("å…¨é¸ éAI", key="nonai__all")
    prev_nonai_all = st.session_state.get("nonai__all_prev", False)
    if non_ai_select_all != prev_nonai_all:
        for display_label in non_ai_display_categories:
            label = f"Non-AI {display_label}"
            if label in unique_cats:
                st.session_state[f"nonai_{display_label}"] = non_ai_select_all
        st.session_state["nonai__all_prev"] = non_ai_select_all

    for display_label in non_ai_display_categories:
        label = f"Non-AI {display_label}"
        if label in unique_cats:
            cnt = (df['Category (20-class)'] == label).sum()
            if st.sidebar.checkbox(
                f"{display_label}ï¼ˆ{cnt} ç¯‡æ–‡ç« ï¼‰",
                key=f"nonai_{display_label}",
                value=st.session_state.get(f"nonai_{display_label}", False)
            ):
                selected_labels.append(label)

    st.sidebar.markdown("---")

    # ===== ä¸»å€å¡Š =====
    search_term = st.text_input("åœ¨Titleæˆ–Subtitleä¸­æœå°‹", key="search_main")

    # å»é‡ä¸¦æ’åºï¼Œé¿å…é †åºå¼•èµ·é ç¢¼é‡ç½®éŒ¯äº‚
    selected_labels = sorted(set(selected_labels))

    # åˆå§‹åŒ– page
    if "page" not in st.session_state:
        st.session_state.page = 0

    # æœå°‹æˆ–é¸æ“‡æ”¹è®Šæ™‚é‡ç½®é ç¢¼
    if st.session_state.get("last_search") != search_term:
        st.session_state.page = 0
        st.session_state.last_search = search_term

    if st.session_state.get("last_selected") != tuple(selected_labels):
        st.session_state.page = 0
        st.session_state.last_selected = tuple(selected_labels)

    if st.session_state.get("last_status") != status_choice:
        st.session_state.page = 0
        st.session_state.last_status = status_choice

    # å…ˆä¾é¡åˆ¥éæ¿¾
    if selected_labels:
        filtered_df = df[df['Category (20-class)'].isin(selected_labels)]
    else:
        filtered_df = df

    # å¥—ç”¨ã€Œç‹€æ…‹ã€ç¯©é¸
    if "åƒ…æœªå ±å‘Š" in status_choice:
        filtered_df = filtered_df[~filtered_df["Read"]]
    elif "åƒ…å·²å ±å‘Š" in status_choice:
        filtered_df = filtered_df[filtered_df["Read"]]

    # æ™‚é–“æ’åº
    filtered_df = filtered_df.sort_values('Date', ascending=False)

    # æœå°‹ï¼ˆå¿½ç•¥å¤§å°å¯«ï¼Œè™•ç† NaNï¼‰
    if search_term:
        mask = (
            filtered_df['Title'].astype(str).str.contains(search_term, case=False, na=False) |
            filtered_df['Subtitle'].astype(str).str.contains(search_term, case=False, na=False)
        )
        search_df = filtered_df[mask]
        st.subheader(f"æœå°‹çµæœï¼ˆå…± {len(search_df)} ç­†ï¼‰")
        if not search_df.empty:
            display_paginated_articles(search_df)
        else:
            st.info("ç„¡ç¬¦åˆæœå°‹çµæœã€‚")
    else:
        display_paginated_articles(filtered_df)
